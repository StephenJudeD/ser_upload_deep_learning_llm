# Audio Emotion Recognition Project

## Introduction
The Audio Emotion Recognition Project aims to analyze audio recordings to detect and interpret the emotions expressed within them. This project leverages state-of-the-art machine learning techniques and a variety of datasets to build robust models capable of understanding emotional nuances in speech. 

The journey of this project begins with data collection and preprocessing, leading to feature extraction and augmentation techniques, which culminate in the development of a machine learning model that can accurately classify emotions in audio data.

## Datasets
The foundation of this project lies in the diverse datasets used for training our models. The following datasets are utilized:

- **RAVDESS**: The Ryerson Audio-Visual Database of Emotional Speech and Song provides a rich array of emotional speech and song recordings, which serve as a primary resource for training emotion recognition models.
- **SAVEE**: The Surrey Audio-Visual Expressed Emotion dataset includes recordings of male actors expressing different emotions, contributing to the model's understanding of emotional variation in male voices.
- **TESS**: The Toronto Emotional Speech Set features emotional recordings from female actors, helping balance the dataset and enhancing the model's capability to recognize emotions across genders.
- **CREMA-D**: A crowd-sourced emotional multimodal actors dataset that includes a wide range of emotions, providing additional context and diversity in emotional expression.

## Cross Corpora Validation
- **EmoDB**: The Emotional Speech Database (German) adds further linguistic diversity and emotional contexts, allowing the model to generalize better across different languages and cultural expressions.
